Title         : DATA 621 - Homework 4
Author        : Honey Berk, Ken Markus
Logo          : False
Title Note  : &date;

[TITLE]
   
# Data Exploration

**Dataset**
{ font-size: 120%; }

The dataset under analysis this week contains information on customers for an auto insurance corporation. The dataset contains more than 8000 records, there are actually two response variables we will be exploring: (1) TARGET_FLAG, which denotes whether or not the customer was in a crash; and, (2) TARGET_AMT, which denotes the dollar-value of the cost of the crash. There are 24 predictor variables included in the training dataset:
~ Center

|Name     | Definition            | Theoretical Effect|
|:--------+:-----------+:-----------------------------|
|AGE      |age of driver          |Very young people tend to be risky. Maybe very old people also.|
|BLUEBOOK |Value of Vehicle       |Unknown effect on probability of collision, but probably effect the payout if there is a crash|
|CAR_AGE| Vehicle Age             | Unknown effect on probability of collision, but probably effect the payout if there is a crash
|**CAR_TYPE**| Type of Car            | Unknown effect on probability of collision, but probably effect the payout if there is a crash
|**CAR_USE**| Vehicle Use             |Commercial vehicles are driven more, so might increase probability of collision
|CLM_FREQ| # Claims (Past 5 Years)| The more claims you filed in the past, the more you are likely to file in the future
|**EDUCATION**| Max Education Level   | Unknown effect, but in theory more educated people tend to drive more safely
|HOMEKIDS| # Children at Home     | Unknown effect
|HOME_VAL| Home Value             | In theory, home owners tend to drive more responsibly
|INCOME| Income                   |In theory, rich people tend to get into fewer crashes
|**JOB**| Job Category                | In theory, white collar jobs tend to be safer
|KIDSDRIV| # Driving Children     |When teenagers drive your car, you are more likely to get into crashes
|**MSTATUS**| Marital Status          |In theory, married people drive more safely
|MVR_PTS| Motor Vehicle Record Points| If you get lots of traffic tickets, you tend to get into more crashes
|OLDCLAIM| Total Claims (Past 5 Years)| If your total payout over the past five years was high, this suggests future payouts will be high
|**PARENT1**| Single Parent           |Unknown effect
|**RED_CAR**| A Red Car               |Urban legend says that red cars (especially red sports cars) are more risky. Is that true?
|**REVOKED**| License Revoked (Past 7 Years)| If your license was revoked in the past 7 years, you probably are a more risky driver.
|**SEX**| Gender                      |Urban legend says that women have less crashes then men. Is that true?
|TIF| Time in Force               |People who have been customers for a long time are usually more safe.
|TRAVTIME| Distance to Work       |Long drives to work usually suggest greater risk
|**URBANICITY**| Home/Work Area       |Unknown
|YOJ| Years on Job                |People who stay at a job for a long time are usually more safe         |                 |
|---------|-----------------|
~
An initial look at the data confirmed that there were a lot of categorical variables, including one of the response variables, TARGET_FLAG. The categorical variables are denoted in **bold** above.

Summary statistics were generated for the dataset:
~ Center
![Screen Shot 2016-07-09 at 3.09.08 PM]

[Screen Shot 2016-07-09 at 3.09.08 PM]: images/Screen-Shot-2016-07-09-at-3.09.08-PM.png "Screen Shot 2016-07-09 at 3.09.08 PM" { width:auto; max-width:90% }
~

Some initial data cleaning was performed to set categorical data to factors and thereby enable a better look at th data.

Summary statistics were generated again:
~ Center
![3-summary]

[3-summary]: images/3-summary.PNG "3-summary" { width:auto; max-width:90% }
~
We can see that the data for each customer record was not complete, and that there were a significant number of NAs for the following predictor variables: YOJ, INCOME, HOME_VAL, JOB and CAR_AGE.

Boxplots and histograms were  generated to examine the distributions of each of the variables. Note that this was subsequent to some of the data cleanup described in the following section. While we planned to apply logistic regression to the first model (the probability of a crash) and data for logistic regression is not required to be normalized, the boxplots and histograms did show that there were a number of variables with highly skewed data, and a large amount of outliers, most notably, OLDCLAIM and INCOME. The response variable, __TARGET_AMT__, also had a large number of outliers.

~ Center
![4-boxplots]

[4-boxplots]: images/4-boxplots.PNG "4-boxplots" { width:auto; max-width:90% }


![5-histograms]

[5-histograms]: images/5-histograms.PNG "5-histograms" { width:auto; max-width:90% }
~
Some additional data prep was during at this point (see Data Preparation section, below, for details.), and a correlation matrix was generated to help identify pairwise correlations amongst the predictors, and also to identify the predictors that were most highly correlated to the response variable, _**TARGET_FLAG**_. 

The exact output of high correlations (>0.5) also follows below:

|Var1     |Var2     | Freq      |
|:--------|:--------|----------:|
|SEX      |RED_CAR  | -0.9469796|
|PARENT1  |MSTATUS  | 0.8264202 |
|SEX      |CAR_TYPE | 0.6383660 |
|JOB      |CAR_USE  |-0.6354292 |
|INCOME   |HOME_VAL | 0.5779975 |
|KIDSDRIVE   | HOMEKIDS| 0.5662063 |
|HOME_VAL |MSTATUS  | -0.5274086|
|CAR_TYPE|RED_CAR|-0.5065342 |

Since the variable inflation factor (VIF) is not applicable to logistic regression, we can use these pairwise correlations later, to try and ensure that we avoid multicollinearity. There are a number of interesting findings here, a few of which follow: 

First, we see that red cars are nearly exclusively purchased by men. Further, by definition, we would think a single parent is unmarried, so we should not be surprised with this correlation (if anything it ought to be higher). Interestingly, the type of car purchased also depends a great deal on gender of the person making the purchase. Finally, the relationship between home values and income should not be a surprise, nor marital status as a married couple increases the likelihood of a double-income household, and thus the ability to afford a more expensive home.

Following is a look at the correlations between the response and predictor variables:
~ Center
![6-responsecorrs]

[6-responsecorrs]: images/6-responsecorrs.PNG "6-responsecorrs" { width:auto; max-width:90% }
~
The lack of strong correlation with any of the predictor variables and the response indicates that we may not have the strongest model as we dig in.

# Data Preparation

The original dataset had quite a few problems:
![Screen Shot 2016-07-09 at 4.19.49 PM]

[Screen Shot 2016-07-09 at 4.19.49 PM]: images/Screen-Shot-2016-07-09-at-4.19.49-PM.png "Screen Shot 2016-07-09 at 4.19.49 PM" { width:auto; max-width:90% }

Dollar-amount values were stored as character strings and included the '\$'-sign and commas. We converted all of those to appropriate numerical types. As mentioned, there were a lot of categorical variables as well (see the **bold** notated variables in the table in the previous section). All of these were converted to the `factor` data-type and the levels were renamed where appropriate. 

We will detail in our model section exactly how we handled the NAs. The cleaned up data are as follows:
~ Center
![7-str]

[7-str]: images/7-str.PNG "7-str" { width:auto; max-width:90% }
~
- We converted a small number of negative values in CAR_AGE_

- We opted to assume that whether or not kids were driving the car was more significant than the number of kids driving. Thus, we converted from an integer to a binary dataset.

~ Center
![kids_drive_hist]
![kids_drive_boxplot]

[kids_drive_hist]: images/kids_drive_hist.png "kids_drive_hist" { width:auto; max-width:90% }

[kids_drive_boxplot]: images/kids_drive_boxplot.png "kids_drive_boxplot" { width:auto; max-width:90% }
~

- We applied the same technique to motor vehicle points.

~ Center
![mvr_pts_plot]

[mvr_pts_plot]: images/mvr_pts_plot.png "mvr_pts_plot" { width:auto; max-width:90% }

![8-points]

[8-points]: images/8-points.PNG "8-points" { width:auto; max-width:90% }
~
We created some new variables, but they seemed to complicate or decrease the effectiveness of the models. One was HOME_OWN, which signified whether the driver was a homeowner or now. This was done by using the HOME_VAL variable and turning into a binary for "don't own" if HOME_VAL was zero, and "own" if HOME_VAL was any other value. We also created an average claim variable, but this was not used in modeling either.

Lastly, for at least the first portion (predicting whether or not a claim is filed), we opted to simply remove all the NAs. The summary output prior to logistic regression was as follows:

~ Center
![9-summary-premodel]

[9-summary-premodel]: images/9-summary-premodel.PNG "9-summary-premodel" { width:auto; max-width:90% }
~

# Models

## Logistic Regression Models
For the first logistic regression, we built a model that included the predictors that were the most highly-correlated with the response, TARGET_FLAG, when we generated the correlation matrix during the data exploration process. 

The AIC for this model was 5797.8, and the residual deviance was 5777.8. The predictors included were:

KIDSDRIV + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + URBANICITY

For the second logistic regression, we used a stepwise:backwards procedure, removing non-significant predictors after each round until we achieved the best AIC. The resulting best model has an AIC of 5463.5 and the residual deviance was 5397.5. The predictors included were:

KIDSDRIV + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + URBANICITY

The third model resulted from using the 'glmulti' package for auto-selection. The AIC was 6299.6 and the residual deviance was 6269.6. The predictors included were:

KIDSDRIV + PARENT1 + MSTATUS + EDUCATION + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + CLM_FREQ

Suprisingly, this was not the best model out of the three. The best was the stepwise:backwards model generated manually.

An analysis was then done to identify influential observations in this model, and three records were removed (3508, 3240, 2808). The model was then re-run, and the resulting AIC was 5447.1 with residual deviance of 5381.1. The final coefficient values can be viewed in the following summary:
~ Center

~

The following table summarizes selected analytics:

~ Center
|Residual          |AIC      |BIC       |AUC    |McFadden's    |
|Deviance          |         |          |       |Pseudo $R^{2}$|
+-----------------:+--------:+---------:+------:+-------------:+
| 5777.8 on 6031 DF |5797.828 | 5864.891 |0.7786 |0.1729838  |
| 5397.5 on 6008 DF|5463.470 |5684.779  |0.8158 |0.2274268     |
|5381.1 on 6005 DF|5447.139 |5668.431  |0.8167 |0.2288845     |
~

Model 3 is the chosen model.

##TARGET_AMT models
###Simple linear model
We limited our dataset further to only include observations wherein a claim was accepted and paid (`df_crash <- df_train[df_train$TARGET_AMT > 0,]`. This brought down the number of observations to 1703.
We removed the `TARGET_FLAG` variable as well.

We opted to apply a simple linear model for our first attempt and this yielded the following results:

![Screen Shot 2016-07-10 at 7.14.45 PM]

[Screen Shot 2016-07-10 at 7.14.45 PM]: images/Screen-Shot-2016-07-10-at-7.14.45-PM.png "Screen Shot 2016-07-10 at 7.14.45 PM" { width:auto; max-width:90% }

This is not a convincing model at all, and upon review of the residuals we recognized that the residual variance was not constant.

![Residuals_lm1]

[Residuals_lm1]: images/Residuals_lm1.png "Residuals_lm1" { width:auto; max-width:90% }

Our first assumption was that this is due to skew of the response variable, and thus the next section attempted to reduce that problem.

We also did review the most influential observations by generating a Cook's D-plot.

![Dplot]

[Dplot]: images/Dplot.png "Dplot" { width:auto; max-width:90% }

This identified 3 observations that we subsequently removed from our models.

We reapplied the linear model and after a few iterations optimized with the following:

![Screen Shot 2016-07-10 at 10.53.28 PM]

[Screen Shot 2016-07-10 at 10.53.28 PM]: images/Screen-Shot-2016-07-10-at-10.53.28-PM.png "Screen Shot 2016-07-10 at 10.53.28 PM" { width:auto; max-width:90% }

The residual error was 7645 on 1618 df. The visuals are as follows:

![lm_plots]

[lm_plots]: images/lm_plots.png "lm_plots" { width:auto; max-width:90% }

In addition to attempting the Box-Cox method, given the heteroskedacity we will leverage WLS in the third part as well.

###Box-Cox Model
We built two models leveraging the Box-cox methodology and unfortunately they achieved little in terms of yielding a superior model (based on Adjusted R-squared or F-statistic). However we did improve the challenges with residual variance.

![Screen Shot 2016-07-10 at 7.21.38 PM]

[Screen Shot 2016-07-10 at 7.21.38 PM]: images/Screen-Shot-2016-07-10-at-7.21.38-PM.png "Screen Shot 2016-07-10 at 7.21.38 PM" { width:auto; max-width:90% }
![BC_residuals]

[BC_residuals]: images/BC_residuals.png "BC_residuals" { width:auto; max-width:90% }

Given the problems with this model, our last attempt was to leverage weighted least squares.

###Weighted Least Squares Model
Let us recall where we were with our original linear model.

![lm_model_graphs]

[lm_model_graphs]: images/lm_model_graphs.png "lm_model_graphs" { width:auto; max-width:90% }
Our residuals as the model extends moves further and further away for being normally desributed. This is clearly due to the ourliers highlighted in the leverage graph above.

We utilize the `MASS` package and the `rlm` function to mitigate against this.

We ran many iterations an ultimately found that the best results came via the bi-squared methodology with the following variables:

![Screen Shot 2016-07-10 at 7.35.27 PM]

[Screen Shot 2016-07-10 at 7.35.27 PM]: images/Screen-Shot-2016-07-10-at-7.35.27-PM.png "Screen Shot 2016-07-10 at 7.35.27 PM" { width:auto; max-width:90% }

We made a few more data manipulations for one final attempt. Specifically, we created a new column `avg_claim` which divided old claims amount from the claims frequency (with the conditional that this was greater than zero). This proved to improved our model, based on minimizing residual errors.

![Screen Shot 2016-07-10 at 11.36.09 PM]

[Screen Shot 2016-07-10 at 11.36.09 PM]: images/Screen-Shot-2016-07-10-at-11.36.09-PM.png "Screen Shot 2016-07-10 at 11.36.09 PM" { width:auto; max-width:90% }

![Model7]

[Model7]: images/Model7.png "Model7" { width:auto; max-width:90% }

# Model Selection & Prediction

Here are the analytics for the three logistic regression models:

~ Center
|Residual          |AIC      |BIC       |AUC    |McFadden's    |
|Deviance          |         |          |       |Pseudo $R^{2}$|
+-----------------:+--------:+---------:+------:+-------------:+
| 5777.8 on 6031 DF |5797.828 | 5864.891 |0.7786 |0.1729838  |
| 5397.5 on 6008 DF|5463.470 |5684.779  |0.8158 |0.2274268     |
|5381.1 on 6005 DF|5447.139 |5668.431  |0.8167 |0.2288845     |
~

Model 3 is the chosen model, for lowest AIC, BIC and residual deviance, and highest AUC and McFadden's Pseudo $R^{2}$.

This models was then used to predict the probability values for TARGET_FLAG based upon the evaluation data supplied.

Here is a summary for the TARGET_FLAG response probabilities in the predicted model:

|Min.       |1st Qu.   |Median     |Mean       |3rd Qu.    |Max.       |
|-|-|-|-|-|-|
| 0.002585 | 0.075480| 0.226200 | 0.274000 | 0.425600 | 0.953500 |

And here are the final coefficients:

~ Center
![12-coeffinal]

[12-coeffinal]: images/12-coeffinal.PNG "12-coeffinal" { width:auto; max-width:90% }
~

And here is the relative importance of each of the coefficients:

~ Center
![14-imp]

[14-imp]: images/14-imp.PNG "14-imp" { width:auto; max-width:60% }
~

We can see that the location predictor, URBANICITY(Highly Rural / Rural), was the most important in this model by far; followed by CAR_USE (Private), REVOKED (Yes) and CAR_TYPE (Sports), just to name the first few. From these, we could get a profile of a driver most likely to have an accident living in a rural area, driving for personal purposes, who has had his license revoked in the past and drives a sports car.

For the TARGET_AMT we discussed the various models, we opted to leverage minimizing the Residual Errors when determining the best model.

~ Center
|Residual          |Model      |
|Standard Error          |         |
+-----------------:+--------:+
| 2118 on 1629 DF |Weighted Least Squares |
| 7645 on 1618 DF|Linear Model |
|2237 on 1693 DF|bisquare regression|
~

The summary for the TARGET_AMT is as follows:
![Screen Shot 2016-07-10 at 11.44.38 PM]

[Screen Shot 2016-07-10 at 11.44.38 PM]: images/Screen-Shot-2016-07-10-at-11.44.38-PM.png "Screen Shot 2016-07-10 at 11.44.38 PM" { width:auto; max-width:90% }

And below is the box plot, it should not be a surprise that the variation is lower for this piece relative to the training data.

![PredictedAmts]

[PredictedAmts]: images/PredictedAmts.png "PredictedAmts" { width:auto; max-width:90% }

# Discussion

For the logistic regression model, the sub-categories of the categorical variables were affecting the model very differently, To really get a quality model, each sub-category would have to be split into its own variable, and then the models run with these sub-category variables. So in a category like education, the effect of Ph.D. could stay in the model, which the other categories, like Bachelor's, could be removed so that the model could be improved. More granular data would provide for more exact models, and then driver profiles could be generated, like the one above.

Although prediction probabilities were generated here, the model is not as good as we would have liked, so we are not as confident as we would like to be in its ability to predict.


For the linear regression the model clearly requires further investigation. The most telling aspect is the lack of normality that remains with the Q-Q plot. It seems likely that extreme dollar amounts as far as outlay payments require either a separate model or additional data that we are missing. One hypothesis to investigate would be liabilities around personal damange beyond that of the car.
