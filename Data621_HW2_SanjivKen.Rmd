---
title: "Data621_HW2Sanjive"
author: "sanjivek"
date: "June 22, 2016"
output: html_document
---

# Q2
```{r, echo = F, warning = F}
library(dplyr)
library(readr)
library(knitr)
library(caret)
library(grid)
library(gridExtra)
library(pROC)
library(reshape2)


# read classification data
raw=read.csv("https://raw.githubusercontent.com/ksanju0/DATA-621/master/classification-output-data.csv", sep = ",", header= TRUE)
str(raw)
head(raw)

raw$class <- as.factor(raw$class)
raw$scored.class <- as.factor(raw$scored.class)

classification <- raw
str(classification)
summary(classification)

# confusion matrix
confusion <- table(classification$class, classification$scored.class)
grid.table(confusion)

cat("The predicted class is horizontal; the actual class is vertical.")

cm <- matrix(c("True Negative", "False Positive", "False Negative", "True Positive"), nrow = 2, byrow = T)
colnames(cm) <- c("Predicted NO", "Predicted YES")
rownames(cm) <- c("Actual NO", "Actual YES")
grid.table(cm)
```

## Q3 Accuracy
```{r, echo = F, warning = F}
actual <- classification$class
predicted <- classification$scored.class
probability <- classification$scored.probability


df_class <- melt(data.frame(actual, predicted,probability))
df_class
colnames(df_class) <- c("actual", "predicted")
head(df_class)


accuracy.fn <- function(classification, actual, predicted) {
  confusion <- table(df_class$actual, df_class$predicted)
  accuracy <- (confusion[2,2] + confusion [1,1]) / sum(confusion)
  return(accuracy)
}

accuracy.fn(df_class, df_class$actual, df_class$predicted)
```


## Q4 Classification Error Rate
```{r, echo = F, warning = F}
error_rate.fn <- function(classification, actual, predicted) {
  confusion <- table(df_class$actual, df_class$predicted)
  error_rate <- (confusion[1,2] + confusion [2,1]) / sum(confusion)
  return(error_rate)
}

error_rate.fn(df_class, df_class$actual, df_class$predicted)
```


# Q5 Precision
```{r, echo = F, warning = F}

precision.fn <- function(classification, actual, predicted) {
  confusion <- table(df_class$actual, df_class$predicted)
  precision <- confusion[2,2] / (confusion[2,2] + confusion[1,2])
  return(precision)
}

precision.fn(df_class, df_class$actual, df_class$predicted)
```


# Q6 Sensitivity
```{r, echo = F, warning = F}

sensitivity.fn <- function(classification, actual, predicted) {
  confusion <- table(df_class$actual, df_class$predicted)
  sensitivity <- confusion[2,2] / (confusion[2,2] + confusion[2,1])
  return(sensitivity)
}

sensitivity.fn(df_class, df_class$actual, df_class$predicted)
```


# Q7 Specificity
```{r, echo = F, warning = F}

specificity.fn <- function(classification, actual, predicted) {
  confusion <- table(df_class$actual, df_class$predicted)
  specificity <- confusion[1,1] / (confusion[1,1] + confusion[1,2])
  return(specificity)
}

specificity.fn(df_class, df_class$actual, df_class$predicted)
```


# Q8 F1 Score
```{r, echo = F, warning = F}

f1score.fn <- function(classification, actual, predicted) {
  precision <- precision.fn(df_class, df_class$actual, df_class$predicted)
  sensitivity <- sensitivity.fn(df_class, df_class$actual, df_class$predicted)
  f1score <- (2 * precision * sensitivity) / (precision + sensitivity)
  return(f1score)
}

f1score.fn(df_class, df_class$actual, df_class$predicted)
```


# Q9 
####Before we move on, let’s consider a question that was asked: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1. (Hint: I 0 < 𝑎 < 1 and 0 < 𝑏 < 1 then 𝑎𝑏 < 𝑎.) 

###Answer:
####Yes, The F1 score is bound by 0 and 1. as per F1 Score formula:

####$$ F1\ Score = \frac{2*P*S }{P + S} $$
####1) $precision= TP/(TP+FP)$, precision will be maximised if we will keep TP=1 and FP=0
####2) $sensitivity = (TP/TP+FN)$, sensitivity to be maximized TP=1 and FN=0

####if precision=p and sensitivity=s, then
####$0<p<1$ and 0<s<1 then, $0<p*s<s$ and$0<p*s<s$
####adding them, we will get
####$0<2p*s<p+s$
####$0<2p*s/(p+s)<1$

####which gives the F1 Score formula:
####$0<F1 Score<1 i.e. F1 score is bound by 0 and 1.




# Q10 ROC Function

####Write a function that generates an ROC curve from a data set with a true classification column (class in our example) and a probability column (scored.probability in our example). Your function should return a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals. 


####Area calculated is:

```{r}
library(zoo)
ROCf <- function(raw) {

refData <- raw[,c(9,10,11)]

thresh <- seq(0.01, 1, by = .01) # this generates the threshold value from 0 to 1 increament of 0.01
Speci <- rep(0.0, 100) # initialize a vetor for specificity
Sensi <- rep(0.0, 100) # initialize a vector for sensitivity

for(k in 1:100) {      #  Loop to generate 100 threshold values vector
    thresh.pred <- rep(0, nrow(refData)) # initialize the threshold vector
    thresh.pred[refData$scored.probability > thresh[k] ] <- 1 # set thresold predictred to 1 if threshold probablity is > threshold calculated above
    Speci[k] <- 1 - calcSpecificity(refData$class, as.matrix(thresh.pred)) # build sensitivity and specificity vectors based on threshold calculated 
    Sensi[k] <- calcSensitivity(refData$class, as.matrix(thresh.pred))

   # if (thresh[k] == 0.5) {  # filter all the Specificity and sensitivity for threshold equal to 0.5 for plotting
  #    SpeciNew <- Speci[k]  
   #   SensiNew <- Sensi[k]
    #}

} 



  plot(Speci , Sensi, xlab = "1 - Specificity", ylab = "Sensitivity", main = "ROC Chart", col = "blue") # plot the graph based on calculated sensitivity and specificity

  #Area calculated is:
  AUC <- -1*sum(diff(na.omit(Speci))*rollmean(na.omit(Sensi),2))  # this calculated the area  under the curve 
  cat("Area under the curve  =", AUC) # prints area under the curve

}

# function to calculate sensitivity with assigned  thresholds

calcSensitivity <- function(actual, predicted) {

        c.mat <- data.frame(table(actual, predicted))
        TN <- as.numeric(as.character(c.mat[1,3]))
        FN <- as.numeric(as.character(c.mat[2,3]))
        FP <- as.numeric(as.character(c.mat[3,3]))
        TP <- as.numeric(as.character(c.mat[4,3]))

        return( TP / (TP + FN) )
    }  
# function to calculate specificity with assigned  thresholds

calcSpecificity <- function(actual, predicted) {
      c.mat <- data.frame(table(actual, predicted))
      TN <- as.numeric(as.character(c.mat[1,3]))
      FN <- as.numeric(as.character(c.mat[2,3]))
      FP <- as.numeric(as.character(c.mat[3,3]))
      TP <- as.numeric(as.character(c.mat[4,3]))
    return( TN / (TN + FP) )
    }  



```

####You can plot the ROC Curve using the below script and passing the dataset: 

```{r}


ROCf(classification)



```

# Q11 Function values
```{r, echo = F, warning = F}
cat("Accuracy =", accuracy.fn(classification, actual, predicted))
cat("Classification Error Rate =", error_rate.fn(classification, actual, predicted))
cat("Precision =", precision.fn(classification, actual, predicted))
cat("Sensitivity =", sensitivity.fn(classification, actual, predicted))
cat("Specificity =", specificity.fn(classification, actual, predicted))
cat("F1 Score =", f1score.fn(classification, actual, predicted))

```





#Q12 Investigate Caret Package 
####Investigate the caret package. In particular, consider the functions confusionMatrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions? 

```{r}
library(caret)

confusionMatrix(factor(classification$scored.class),factor(classification$class),positive="1")


```
Comparing with our function as :

| Approach       |  Sensitivity|  Specificity |  Accuracy  
|----------------|-------------|--------------|------------
| Our Functions  |    0.4737   |   0.9597     |   0.8066    
| caret package  |    0.4737   |   0.9597     |   0.8066   



#Q13  Investigate pROC Package
####Investigate the pROC package. Use it to generate an ROC curve for the data set. How do the results compare with your own functions? 
```{r}
library(pROC)

rocP <- roc(factor(class) ~ scored.probability, data=classification)

plot(rocP)

```
####Comparing the custom function with the pROC Package as:

  Approach                  |  AUC
|---------------------------|---------
| Custom Fuction            |  0.8247
| pROC Package              |  0.8503 





