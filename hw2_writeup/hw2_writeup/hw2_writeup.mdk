Title         : DATA 621 - Homework 2
Author        : Honey Berk, Sanjiv Kumar, Ken Markus
Logo          : False
Title Note  : &date;

[TITLE]

# Confusion Matrix { - }
Using the predicted (scored.class) and actual (class) data from the classification data, the following confusion matrix was generated:
~ Center
| |0  |1|
+-:+---+--:+
|0|119|30|
|1|5  |27|
~
In this confusion matrix, the predicted data is in rows, while the actual data is in columns. The following matrix depicts the meaning of this confusion matrix:
~ Center
| |Actual - Negative|Actual - Positive|
+-------------------:+---------------:+---------:+
|Predicted - Negative|True Negative (TN)|False Negative (FN)|
|Predicted - Positive|False Positive (FP)|True Positive (TP)|
~

# R Functions for Calculated Rates { - }
Write R functions that take the data set as a dataframe, with actual and predicted classifications identified, and return predictive analytics, as follows:

## Accuracy { - } 
Overall, how often is the classifier correct?
~ Math
Accuracy = \frac{TP + TN}{TP + FP + TN + FN}
~
_R Code:_
``` { .pretty }
  # accuracy
  accuracy.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  accuracy <- (confusion[2,2] + confusion [1,1]) / sum(confusion)
  return(accuracy)
}
```
## Classification Error Rate { - }
Overall, how often is the classifier wrong?
~ Math
Classification Error Rate = \frac{FP + FN}{TP + FP + TN + FN}
~
_R Code:_
``` { .pretty }
  # classification error rate
  error_rate.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  error_rate <- (confusion[2,1] + confusion [1,2]) / sum(confusion)
  return(error_rate)
}
```
## Verify Accuracy and Error Rate { -}
Accuracy = 0.8066298, Error Rate = 0.1933702
~ Math
0.8066298 + 0.1933702 = 1
~
So, accuracy and error rate sum to one. 
## Precision
When it predicts yes, how often is it correct?
~ Math
Precision = \frac{TP}{TP + FP}
~
_R Code:_
``` { .pretty }
precision.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  precision <- confusion[2,2] / (confusion[2,2] + confusion[2,1])
  return(precision)
}
```
## Sensitivity { - }
When it's actually yes, how often does the classifier predict yes?
~ Math
Sensitivity = \frac{TP}{TP + FN}
~
_R Code:_
``` { .pretty }
sensitivity.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  sensitivity <- confusion[2,2] / (confusion[2,2] + confusion[1,2])
  return(sensitivity)
}
```
## Specificity { - }
When it's actually no, how often does the classifier predict no?
~ Math
Specificity = \frac{TN}{TN + FP}
~
_R Code:_
``` { .pretty }
specificity.fn <- function(df, actual, predicted) {
  cconfusion <- table(predicted, actual)
  specificity <- confusion[1,1] / (confusion[1,1] + confusion[2,1])
  return(specificity)
}
```
## F1 Score { - }
A weighted average of the precision and sensitivity.
~ Math
F1 Score = \frac{2*Precision*Sensitivity}{Precision + Sensitivity}
~
_R Code:_
``` { .pretty }
f1score.fn <- function(df, actual, predicted) {
  precision <- precision.fn(df_class, actual, predicted)
  sensitivity <- sensitivity.fn(df_class, actual, predicted)
  f1score <- (2 * precision * sensitivity) / (precision + sensitivity)
  return(f1score)
}
```
## Bounds on F1 Score { - }
Show that the F1 score will always be between 0 and 1.

* As stated above, the F1 score is the weighted average of the precision (P) and sensitivity (S).

* Precision is bounded by 0 and 1 because it represents the percentage (ratio) of true positives out of the sum of true positives plus false positives.

* Sensitivity is bounded by 0 and 1 because it represents the percentage (ratio) of true positives out of the sum of true positives and false negatives.

* Even if precision and sensitivity are equal to the maximum value of 1, the F1 score will never be greater than one: $\frac{2*1*1}{1+1} = 1$. If precision and sensitivity are both equal to the minimum value of 0, then the F1 score will be equal to 0: $\frac{2*0*0}{0+0} = 0$ (this is true if either of the values is equal to 0). As an example, using some other values of P and S: $\frac{2*0.5*0.5}{0.5+0.5} = 0.5$, $\frac{2*0.75*0}{0.75+0} = 0$, etc.

# ROC Curve Function 

# Classification Metrics { - }
## R Functions vs. Caret Package { - }

| | |Accuracy|Error Rate|Precision|Sensitivity|Specificity|F1 Score|
+-+-+-------:+---------:+--------:+----------:+----------:+-------:+
|**R Functions**|0.8066|0.1934|0.8438|0.4737|0.9597|0.6067|
|**Caret**|0.8066|          |0.8438|0.4737|0.9597|        |


