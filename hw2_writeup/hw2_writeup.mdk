Title         : DATA 621 - Homework 2
Author        : Honey Berk, Sanjiv Kumar, Ken Markus
Logo          : False
Title Note  : &date;

[TITLE]

# Confusion Matrix { - }
Using the predicted (scored.class) and actual (class) data from the classification data, the following confusion matrix was generated:
~ Center
| |0  |1|
+-:+---+--:+
|**0**|119|30|
|**1**|5  |27|
~
In this confusion matrix, the predicted data is in rows, while the actual data is in columns. The following matrix depicts the meaning of this confusion matrix:

| |Actual - Negative|Actual - Positive|
+:-------------------+:---------------+:---------+
|**Predicted - Negative**|True Negative (TN)|False Negative (FN)|
|**Predicted - Positive**|False Positive (FP)|True Positive (TP)|


# R Functions for Calculated Rates { - }
Write R functions that take the data set as a dataframe, with actual and predicted classifications identified, and return predictive analytics, as follows:

## Accuracy { - } 
Overall, how often is the classifier correct?
~ Math
Accuracy = \frac{TP + TN}{TP + FP + TN + FN}
~
_R Code:_
``` { .pretty }
  # accuracy
  accuracy.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  accuracy <- (confusion[2,2] + confusion [1,1]) / sum(confusion)
  return(accuracy)
}
```
## Classification Error Rate { - }
Overall, how often is the classifier wrong?
~ Math
Classification Error Rate = \frac{FP + FN}{TP + FP + TN + FN}
~
_R Code:_
``` { .pretty }
  # classification error rate
  error_rate.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  error_rate <- (confusion[2,1] + confusion [1,2]) / sum(confusion)
  return(error_rate)
}
```
## Verify Accuracy and Error Rate { -}
Accuracy = 0.8066298, Error Rate = 0.1933702
~ Math
0.8066298 + 0.1933702 = 1
~
So, accuracy and error rate sum to one.

## Precision { -}
When the classifier predicts yes, how often is it correct?
~ Math
Precision = \frac{TP}{TP + FP}
~
_R Code:_
``` { .pretty }
precision.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  precision <- confusion[2,2] / (confusion[2,2] + confusion[2,1])
  return(precision)
}
```
## Sensitivity { - }
When it's actually yes, how often does the classifier predict yes?
~ Math
Sensitivity = \frac{TP}{TP + FN}
~
_R Code:_
``` { .pretty }
sensitivity.fn <- function(df, actual, predicted) {
  confusion <- table(predicted, actual)
  sensitivity <- confusion[2,2] / (confusion[2,2] + confusion[1,2])
  return(sensitivity)
}
```
## Specificity { - }
When it's actually no, how often does the classifier predict no?
~ Math
Specificity = \frac{TN}{TN + FP}
~
_R Code:_
``` { .pretty }
specificity.fn <- function(df, actual, predicted) {
  cconfusion <- table(predicted, actual)
  specificity <- confusion[1,1] / (confusion[1,1] + confusion[2,1])
  return(specificity)
}
```
## F1 Score { - }
A weighted average of the precision and sensitivity.
~ Math
F1 Score = \frac{2*Precision*Sensitivity}{Precision + Sensitivity}
~
_R Code:_
``` { .pretty }
f1score.fn <- function(df, actual, predicted) {
  precision <- precision.fn(df_class, actual, predicted)
  sensitivity <- sensitivity.fn(df_class, actual, predicted)
  f1score <- (2 * precision * sensitivity) / (precision + sensitivity)
  return(f1score)
}
```
## Bounds on F1 Score { - }
Show that the F1 score will always be between 0 and 1.

* As stated above, the F1 score is the weighted average of the precision (P) and sensitivity (S).

* Precision is bounded by 0 and 1 because it represents the percentage (ratio) of true positives out of the sum of true positives plus false positives.

* Sensitivity is bounded by 0 and 1 because it represents the percentage (ratio) of true positives out of the sum of true positives and false negatives.

* Even if precision and sensitivity are equal to the maximum value of 1, the F1 score will never be greater than one: $\frac{2*1*1}{1+1} = 1$. If precision and sensitivity are both equal to the minimum value of 0, then the F1 score will be equal to 0: $\frac{2*0*0}{0+0} = 0$ (this is true if either of the values is equal to 0). As an example, using some other values of P and S: $\frac{2*0.5*0.5}{0.5+0.5} = 0.5$, $\frac{2*0.75*0}{0.75+0} = 0$, etc.

# ROC Curve Function { - }
In a ROC (Receiver Operating Characteristic) curve, the true positive rate (sensitivity) is plotted against the false positive rate (1-specificity) for different cut-off points. The ROC curve demonstrates the trade-off between sensitivity and specificity (an increase in sensitivity will be accompanied by a decrease in specificity). The closer the curve follows the left-hand border, and then the top border, of the ROC space, the more accurate the test. Also, the closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.

Once the ROC curve is generated, the area under the curve (AUC) can be used as a quantitative measure of performance. The model with the largest area under the ROC curve would be the most effective.

The ROC_AUC function calculated the AUC as: 0.8503.

Here is the code for a function written to plot the ROC curve and calculate the AUC: 

_R Code:_
``` { .pretty }
  ROC_AUC.fn = function(df, probability, actual) {
  probabilitySort = sort(probability, decreasing = T, index.return = T)
  values = unlist(probabilitySort$x)
  indx = unlist(probabilitySort$ix)  
  
  roc_y = actual[indx];
  stack_x = cumsum(roc_y == 0)/sum(roc_y == 0)
  stack_y = cumsum(roc_y == 1)/sum(roc_y == 1)    
  
  auc = sum((stack_x[2:length(roc_y)]-stack_x[1:length(roc_y)-1])*stack_y[2:length(roc_y)])
  return(list(stack_x=stack_x, stack_y=stack_y, auc=auc))
  }

  AUC_list = ROC_AUC.fn(df, probability, actual) 

  stack_x = unlist(AUC_list$stack_x)
  stack_y = unlist(AUC_list$stack_y)
  auc = unlist(AUC_list$auc)
  cat("AUC = ", auc)

  plot(stack_x, stack_y, type = "l", col = "blue", xlab = "False Positives (1 - Specificity)", 
  ylab = "True Positives (Sensitivity)", main = "ROC Curve")
  axis(1, seq(0.0,1.0,0.1))
  axis(2, seq(0.0,1.0,0.1))
  abline(h=seq(0.0,1.0,0.1), v=seq(0.0,1.0,0.1), col="gray", lty=3)
  legend(0.7, 0.3, sprintf("%4.4f",auc), lty=c(1,1), lwd=c(2.5,2.5),  col="blue", title = "AUC")
```
![ROC_AUC-3]

[ROC_AUC-3]: images/ROC_AUC-3.PNG "ROC_AUC-3" { width:auto; max-width:90% }

# Classification Metrics { - }
## R Functions vs. Caret Package { - }

| |Accuracy|Error Rate|Precision|Sensitivity|Specificity|F1 Score|
+-+-:+-------:+---------:+--------:+----------:+----------:+-------:+
|**R Functions**|0.8066|0.1934|0.8438|0.4737|0.9597|0.6067|
|**Caret**[^fn]|0.8066|0.1934|0.8438|0.4737|0.9597|0.6067|

[^fn]: Error rate and F1 score derived from formulas: Error rate: $(1 -Accuracy)$; F1 score: $\frac{2*Precision*Sensitivity}{Precision + Sensitivity}$.

# Caret Package Output { - }
## Confusion Matrix and Statistics { - }
![caret-1]

[caret-1]: images/caret-1.PNG "caret-1" { width:auto; max-width:90% }

# pROC Package { - }
## ROC Curve { - }
A ROC curve graph summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate against the False Positive Rate as you vary the threshold for assigning observations to a given class. 

**Area under the curve (AUC):** 0.8503

**Data:** Probability in 124 controls (actual 0) < 57 cases (actual 1)

![pROC-2]

[pROC-2]: images/pROC-2.PNG "pROC-2" { width:auto; max-width:90% }

The ROC_AUC function calculated the same AUC value as the pROC package:

|ROC_AUC Function|pROC Package|
|AUC Value|AUC Value|
+---------:+-------:+
|0.8503|0.8503|